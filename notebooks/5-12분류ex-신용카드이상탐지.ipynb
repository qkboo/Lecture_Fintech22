{"cells":[{"cell_type":"markdown","metadata":{"id":"a2whf8CNiBff"},"source":["# Autoencoder 이용 이상거래 탐지\n","\n","Keras로 구현된 Autoencoder 신경망을 비지도학습(unsupervised) 또는 반지도학습(semi-supervised) 방식으로 학습하여 신용카드 이상 거래를 탐지해봅니다. 그렇게 학습된 모델은 미리 정답을 알고 있는, 익명화된 데이터셋을 기반으로 평가합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0jcER2wvnptT"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLKCXdbcntKy"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rtuR9stviEq_"},"outputs":[],"source":["from keras.models import Model, load_model\n","from keras.layers import Input, Dense\n","from keras.callbacks import ModelCheckpoint, TensorBoard\n","from keras import regularizers\n","\n","# 시각화 라이브러리 설정\n","%matplotlib inline\n","\n","sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n","\n","plt.rcParams['figure.figsize'] = 14, 8\n","\n","# RANDOM_SEED와 LABELS 설정\n","RANDOM_SEED = 42\n","LABELS = [\"Normal\", \"Fraud\"]"]},{"cell_type":"markdown","metadata":{"id":"mFLnv3nvk_Od"},"source":["### Creditcard fraud 데이터 세트\n","\n","Creditcard fraud 데이터 세트는 2013년 9월 유럽 카드 소지자가 신용카드로 2일 동안 발생한 거래를 보여 주며, 284,807 건의 거래 중 492건의 비정상 사용 데이터가 있다.  데이터 세트는 매우 불균형하며 positive class(Fruad)는 모든 거래의 0.172%를 차지한다. \n","\n","feature 데이터는 기밀 유지 문제로 데이터에 대한 원래 내용과 추가 배경정보는 제공하지 않는다.  컬럼은 V1~ V28로 구성되어, PCA로 한번 가공된 구성요소 이다. PCA 변환 결과 인 숫자 입력 변수만 포함한다. 안타깝게도 기밀 유지 문제로 각 컬럼이 무슨 의미를 하는지는 알 수없다. PCA로 변환되지 않는 유일한 기능은 '시간'과 '양'이다. '시간'에는 각 트랜잭션과 데이터 세트의 첫번째 트랜잭션 사이에 경과 된 시간(초)이 포함된다. '금액',Amount는 거래 금액이며, 이 기능은 예에따라 비용에 민감한 학습에 사용할 수 있다.\n","\n","'Class' 변수는 부정행위를 하면 1이고, 그렇지 않으면 0을 사용한다.\n","\n","- 원본 파일은 2 파일\n","- 파일을 데이터 프레임으로 병합\n"]},{"cell_type":"markdown","metadata":{"id":"ONsnhMS6m0fa"},"source":["### 로컬 파일 이용\n","\n","> 교육을 위해서 github 에 `creditcard_fraud.zip.001` 분할 압축 파일로 제공"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fljELL3skhEz"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ndG__mIFjwbW"},"source":["## 데이터\n","\n","\n","구글 데이터세트 검색에서 \"creditcard fraud\" 를 검색해 신용카드 사기 데이터를 찾는다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZOQUGL4nPPD"},"outputs":[],"source":["pd.options.display.max_columns = 50"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmInVEvVkgeD"},"outputs":[],"source":["df_1 = pd.read_csv('../data/creditcard_1.csv')\n","df_1.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p6ho11jdnUoL"},"outputs":[],"source":["df_1 = df_1.drop('Unnamed: 0', axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKCjwNoGnWn1"},"outputs":[],"source":["df_2 = pd.read_csv('../data/creditcard_2.csv')\n","df_2 = df_2.drop('Unnamed: 0', axis=1)"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"cc7r10vmjwbd"},"source":["### Colab 이용"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ujKJWOInD7b"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["pd.options.display.max_columns = 50"],"metadata":{"id":"C-8KtwhVkkPe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UyvKZL_Cjwbe"},"outputs":[],"source":["df_1 = pd.read_csv('drive/MyDrive/datasets/creditcard_1.csv')\n","df_1.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ZjdiJC1jwbf"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kNaEDmzQjwbf"},"outputs":[],"source":["df_1 = df_1.drop('Unnamed: 0', axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzWMszQdjwbf"},"outputs":[],"source":["df_2 = pd.read_csv('drive/MyDrive/datasets/creditcard_2.csv')\n","df_2 = df_2.drop('Unnamed: 0', axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRdJASKmnbOI"},"outputs":[],"source":["df_1.shape, df_2.shape"]},{"cell_type":"markdown","metadata":{"id":"9voFJR9YndxU"},"source":["### 두 개의 파일을 하나로 합친다.\n","\n","- 2개 이상의 DF를 수직 컬럼 방향으로 결합 시키기 위해서 axis=0을 지정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZrnDOC-Ujwbg"},"outputs":[],"source":["df_1.shape, df_2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lvJWd2blnerG"},"outputs":[],"source":["df = pd.concat([df_1, df_2], axis=0)\n","df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZyC3GSgnlri"},"outputs":[],"source":["del df_1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4wyjCvUnj6Z"},"outputs":[],"source":["del df_2"]},{"cell_type":"markdown","metadata":{"id":"EmV6kBinjwbi"},"source":["# 사기와 정상에 대한 값 확인\n","\n","\n","사기 건에 대한 비율을 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foTkrXGfjwbi"},"outputs":[],"source":["df['Class'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"52EcqO4cjwbi"},"outputs":[],"source":["# 정상-비정상 사용의 비율\n","Fraud = df[df['Class']==1]\n","Valid = df[df['Class']==0]\n","outlier_fraction = len(Fraud)/float(len(Valid))\n","\n","print(f' {outlier_fraction * 100 :.2f} %')"]},{"cell_type":"markdown","metadata":{"id":"e2Kbi6JDjwbi"},"source":["데이터 세트는 매우 불균형하며 positive class(Fruad)는 모든 거래의 0.172%를 차지한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFoIbbYxjwbj"},"outputs":[],"source":["print(\"Fraud Cases : {}\".format(len(Fraud)))\n","print(\"Valid Cases : {}\".format(len(Valid)))"]},{"cell_type":"markdown","metadata":{"id":"CRzDFDxljwbj"},"source":["비정상 비율이 꽤 작다 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7CpQRVmjwbj"},"outputs":[],"source":["sns.countplot(x='Class', data=df)\n","\n","plt.title(\"Transaction class distribution\")\n","plt.xticks(range(2), ['Normal','Fraud'])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"kwUmX5Tqjwbj"},"source":["### 거래 유형별 분석"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KnRb0zQrjwbj"},"outputs":[],"source":["fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n","fig.suptitle('Amount per transaction by class')\n","\n","bins = 50\n","\n","ax1.hist(Fraud.Amount, bins = bins)\n","ax1.set_title('Fraud')\n","\n","ax2.hist(Valid.Amount, bins = bins)\n","ax2.set_title('Normal')\n","\n","plt.xlabel('Amount ($)')\n","plt.ylabel('Number of Transactions')\n","plt.xlim((0, 20000))\n","plt.yscale('log')\n","plt.show();\n"]},{"cell_type":"markdown","metadata":{"id":"4ynfAkgIjwbk"},"source":["이상 거래가 특정 시간에 더 자주 발생하기도 할까요?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"De6e7Pxxjwbk"},"outputs":[],"source":["f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n","f.suptitle('Time of transaction vs Amount by class')\n","\n","ax1.scatter(Fraud.Time, Fraud.Amount)\n","ax1.set_title('Fraud')\n","\n","ax2.scatter(Valid.Time, Valid.Amount)\n","ax2.set_title('Normal')\n","\n","plt.xlabel('Time (in Seconds)')\n","plt.ylabel('Amount')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"pMu22kjzjwbk"},"source":["거래가 발생한 시간이 그렇게 중요한 것 같지는 않아 보입니다."]},{"cell_type":"markdown","metadata":{"id":"-v-g_xyYjwbk"},"source":[]},{"cell_type":"markdown","metadata":{"id":"jIYHXaUPjwbk"},"source":["# Autoencoder\n","\n","'입력값 x를 받아서 다시 입력값 x를 결과로 뱉어내는 함수'가 바로 Autoencoder 신경망이라는 것이죠."]},{"cell_type":"markdown","metadata":{"id":"qgyFMJ82jwbl"},"source":["특정한 데이터의 압축된 표상(representation)을 학습시키고 그 구조(structure)를 찾고 싶어한다는 점이 중요합니다. 이것은 모델 안에 있는 은닉 유닛(hidden unit)의 수를 제한하는 방법을 통해 가능합니다. 이런 종류의 autoencoder들은 미완성(undercomplete) 이라 불립니다.\n","\n","<img src='https://www.bpesquet.fr/mlhandbook/_images/keras_autoencoders_applications.png'>"]},{"cell_type":"markdown","metadata":{"id":"fCWj_UX2jwbl"},"source":["## 훈련 데이터 준비1"]},{"cell_type":"markdown","metadata":{"id":"ymK6JSA2jwbl"},"source":["먼저, 시간 열은 쓰지 않을 것이기 때문에 삭제하고 Scikit-learn의 StandardScaler를 거래액 열에 적용해봅시다. StandardScaler는 평균을 제거하고 값들을 단위 분산(unit variance)에 맞게 스케일링해줍니다."]},{"cell_type":"markdown","metadata":{"id":"RmI_s-7jjwbl"},"source":["딥러닝으로 처리할 때는 모든 칼럼을 정규화나 표준화 필요\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xTDZARS4jwbl"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O__p6Qiojwbm"},"outputs":[],"source":["data = df.drop(['Time'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZl-bDA4jwbm"},"outputs":[],"source":["data['Amount'].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H4LnY8vojwbm"},"outputs":[],"source":["data['Amount'] = scaler.fit_transform(data['Amount'].values.reshape(-1, 1))"]},{"cell_type":"markdown","metadata":{"id":"eoiuIZnkjwbm"},"source":["데이터를 훈련과 테스트 세트로 분리\n","\n","Autoencoder를 훈련하는 것은 우리에게 익숙한 방식과는 조금 다릅니다. 우리가 가진 데이터셋 내에 이상이 없는 거래기록이 대부분이라고 가정해봅시다. 그리고 새로운 거래가 생길 때 어떤 이상이 있으면 그것을 탐지해내고 싶다고 합시다. 그렇다면 우리는 모델을 오직 정상 거래기록으로만 훈련시킵니다. 정상 거래기록을 테스트셋에 넣으면 모델의 성능을 평가할 수 있습니다. 우리는 가진 데이터의 20%를 테스트에 사용할 것입니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7XdvWR-jwbm"},"outputs":[],"source":["X_train, X_test = train_test_split(data, test_size=0.2, random_state=0)\n","\n","X_train = X_train[X_train.Class == 0]\n","X_train = X_train.drop(['Class'], axis=1)\n","\n","y_test = X_test['Class']\n","X_test = X_test.drop(['Class'], axis=1)\n","\n","X_train = X_train.values\n","X_test = X_test.values\n","\n","X_train.shape"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"azgd6gBdjwbm"},"source":["### 훈련 데이터 준비2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LCqmZ5jJjwbn"},"outputs":[],"source":["# X = df.drop(['Time','Class'], axis=1) #피처 데이터\n","# y = df['Class']                            #라벨 데이터\n","# X = df_copy.iloc[:, :-1] #피처 데이터\n","# y = df_copy.iloc[:, -1] #라벨 데이터"]},{"cell_type":"markdown","metadata":{"id":"EIx2fBMjjwbn"},"source":["### 모델 만들기\n","\n","우리가 만들 Autoencoder는 4개의 fully connected layer로 만들어져 있으며, 각 layer는 14, 7, 7, 29개의 뉴런으로 구성되어 있습니다. 처음 두 개의 layer들은 encoder로 쓰이며, 뒤의 두 개는 decoder가 됩니다. 또, L1 정규화(regularization)를 사용합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UTXlJcEijwbn"},"outputs":[],"source":["input_dim = X_train.shape[1]\n","encoding_dim = 14\n","\n","input_layer = Input(shape=(input_dim, ))\n","\n","encoder = Dense(encoding_dim, activation=\"tanh\", \n","                activity_regularizer=regularizers.l1(10e-5))(input_layer)\n","encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n","\n","decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n","decoder = Dense(input_dim, activation='relu')(decoder)\n","\n","autoencoder = Model(inputs=input_layer, outputs=decoder)"]},{"cell_type":"markdown","metadata":{"id":"Wo3l6nr5jwbn"},"source":["우리가 만든 모델을 100 에폭(epoch) 동안 32의 배치 사이즈(batch size)로 훈련시킨 후, 가장 우수한 성능을 가진 모델을 파일로 저장해봅시다. Keras에서 제공되는 ModelCheckpoint가 이러한 일들을 하는 데 매우 편리합니다. 추가적으로, 훈련 과정은 TensorBoard에 맞는 포맷으로 저장됩니다.b"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEQJq1wmjwbo"},"outputs":[],"source":["nb_epoch = 100\n","batch_size = 32\n","autoencoder.compile(optimizer='adam', \n","                    loss='mean_squared_error', \n","                    metrics=['accuracy'])\n","# 체크포인트 모델 저장.\n","checkpointer = ModelCheckpoint(filepath=\"model.h5\",\n","                               verbose=0,\n","                               save_best_only=True)\n","\n","tensorboard = TensorBoard(log_dir='./logs',\n","                          histogram_freq=0,\n","                          write_graph=True,\n","                          write_images=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"M6zVmClGjwbp"},"outputs":[],"source":["%%time\n","history = autoencoder.fit(X_train, X_train,\n","                    epochs=nb_epoch,\n","                    batch_size=batch_size,\n","                    shuffle=True,\n","                    validation_data=(X_test, X_test),\n","                    callbacks=[checkpointer, tensorboard])"]},{"cell_type":"markdown","metadata":{"id":"0Uohlen_jwbp"},"source":["평가"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2uKxk7XSIQdq"},"outputs":[],"source":["# Fashion MNIST 분류 모델 학습 결과 시각화\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 4))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], 'b-', label='loss')\n","plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n","plt.xlabel('Epoch')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['accuracy'], 'g-', label='accuracy')\n","plt.plot(history.history['val_accuracy'], 'k--', label='val_accuracy')\n","plt.xlabel('Epoch')\n","plt.ylim(0.7, 1)\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1rNcLsYmjwbp"},"outputs":[],"source":["plt.plot(history['loss'])\n","plt.plot(history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper right');"]},{"cell_type":"markdown","metadata":{"id":"0Jtm0dlNjwbq"},"source":["오류 분포에 대해 좀 더 자세히 살펴봅시다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEfFkI9tjwbq"},"outputs":[],"source":["predictions = autoencoder.predict(X_test)\n","mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n","error_df = pd.DataFrame({'reconstruction_error': mse,\n","                        'true_class': y_test})\n","error_df.describe()"]},{"cell_type":"markdown","metadata":{"id":"XGpf52Kkjwbq"},"source":["### 거래의 재구성 오류 분포"]},{"cell_type":"markdown","metadata":{"id":"hGB3qoLljwbq"},"source":["정상 거래 데이터들의 재구성 오류 분포"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHxcNwkNjwbr"},"outputs":[],"source":["fig = plt.figure()\n","ax = fig.add_subplot(111)\n","\n","normal_error_df = error_df[(error_df['true_class']== 0) & (error_df['reconstruction_error'] < 10)]\n","_ = ax.hist(normal_error_df.reconstruction_error.values, bins=10)"]},{"cell_type":"markdown","metadata":{"id":"no5kbx17jwbr"},"source":["이상 거래 데이터들의 재구성 오류 분포"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L13WtXZRjwbr"},"outputs":[],"source":["fig = plt.figure()\n","ax = fig.add_subplot(111)\n","fraud_error_df = error_df[error_df['true_class'] == 1]\n","_ = ax.hist(fraud_error_df.reconstruction_error.values, bins=10)"]},{"cell_type":"markdown","metadata":{"id":"y2H7Eufvjwbs"},"source":["### ROC 곡선을 통한 평가"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBjYrZ5ojwbs"},"outputs":[],"source":["from sklearn.metrics import ( confusion_matrix, precision_recall_curve, auc,\n","                             roc_curve, recall_score, classification_report, f1_score,\n","                             precision_recall_fscore_support )"]},{"cell_type":"markdown","metadata":{"id":"9j0qQ9jljwbs"},"source":["ROC 곡선은 이진 분류기(binary classifier)의 성능을 평가하는 유용한 도구입니다. 그러나 우리의 경우는 살짝 일반적이지는 않습니다. 우리는 굉장히 불균형한 데이터셋을 가지고 있기 때문입니다. 그럼에도, 일단은 우리의 ROC 곡선을 살펴봅시다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuze1jTqjwbs"},"outputs":[],"source":["fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)\n","roc_auc = auc(fpr, tpr)\n","\n","plt.title('Receiver Operating Characteristic')\n","plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n","plt.legend(loc='lower right')\n","plt.plot([0,1],[0,1],'r--')\n","plt.xlim([-0.001, 1])\n","plt.ylim([0, 1.001])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"G6NYHr2njwbt"},"source":["ROC 곡선은 각 기준치(threshold)가 달라질 때마다 '틀린 것을 맞다고 할 확률'(false positive rate)에 따른 '맞는 것을 맞다고 한 확률'(true positive rate)을 그래프로 그린 것입니다. 기본적으로, 파란색 곡선이 왼쪽 위 모퉁이에 가까워질수록 좋습니다. 우리의 결과가 꽤 좋아보이긴 하지만, 우리 데이터가 가진 특징을 염두하고 있어야 합니다. ROC 곡선은 여기서 그다지 유용해보이진 않네요."]},{"cell_type":"markdown","metadata":{"id":"OdvzbreVjwbt"},"source":["### 정밀도/재현율\n","\n","정밀도(Precision)와 재현율(Recall)의 예를 들어보면 정보 검색 분야를 예로 들면, ***정밀도*** 는 검색한 결과들 중에서 실제 찾으려는 대상과 관련된 결과가 얼마나 있는지를 수치화합니다. 반대로, ***재현율*** 은 실제 찾으려는 대상과 관련된 전체 데이터 중 얼마나 찾았는지를 수치화합니다. 두 수치 모두 0과 1사이구요. 수치가 1에 가까울수록 좋습니다."]},{"cell_type":"markdown","metadata":{"id":"lv7Nx16cjwbt"},"source":["예시에서 ***재현율이 높지*** 만 <u>정밀도가 낮다는 것은 찾은 데이터의 수가 많지만 그 중 실제 찾으려는 대상의 비율은 낮았다는 것을 의미</u>합니다. 이와 반대로 ***정밀도가 높지*** 만 재현율이 낮다는 것은 찾은 데이터의 수는 작지만 그 중 실제 찾으려는 대상의 비율은 높다는 것을 의미합니다. 이상적으로는 정밀도와 재현율이 모두 높은 것이 좋습니다. 이는 찾은 데이터 수가 많으며 그 중 실제 찾으려는 대상의 비율이 높다는 것을 의미합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sX5Qq6URjwbt"},"outputs":[],"source":["precision, recall, th = precision_recall_curve(error_df.true_class, error_df.reconstruction_error)\n","plt.plot(recall, precision, 'b', label='Precision-Recall curve')\n","plt.title('Recall vs Precision')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"xZAchJsnjwbt"},"source":["곡선 아래 영역 중 위쪽 부분은 높은 재현율과 높은 정밀도를 나타냅니다. 높은 정밀도는 거짓인데 참이라고 예측한 것들의 비율이 낮은 것과 관련 있고, 높은 재현율은 참인데 거짓이라고 예측한 것들의 비율이 낮은 것과 관련 있습니다. 두 값 모두 높다면 해당 분류기가 실제 참인 데이터들 중 대부분을 찾아낼(높은 재현율)뿐만 아니라, 정확한 결과(높은 정밀도)를 가진다는 것을 의미합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-MZv8R5Gjwbu"},"outputs":[],"source":["plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')\n","plt.title('Precision for different threshold values')\n","plt.xlabel('Threshold')\n","plt.ylabel('Precision')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"HPuKdHZfjwbu"},"source":["재구성 오류가 증가할수록 정밀도가 올라간다는 것을 확인할 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"xnoHvXq5jwbu"},"source":["재현율도 살펴보자:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A06yU3EZjwbu"},"outputs":[],"source":["plt.plot(th, recall[1:], 'b', label='Threshold-Recall curve')\n","plt.title('Recall for different threshold values')\n","plt.xlabel('Reconstruction error')\n","plt.ylabel('Recall')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"H12zkGrOjwbu"},"source":["여기에서는 정반대의 상황이 나타납니다. 재구성 오류가 증가할수록 재현율은 낮아집니다."]},{"cell_type":"markdown","metadata":{"id":"eJ8I-LLLjwbu"},"source":["## 예측하기\n","\n","이번에는 우리의 모델이 좀 다릅니다. 우리의 모델은 새로운 값을 어떻게 예측해야 하는지 알지 못합니다. 그러나 우리는 그럴 필요가 없죠. 새로운, 본 적 없는 데이터가 정상 거래인지 이상 거래인지 알기 위해서는 그 데이터에서 나온 재구성 오류를 계산하면 됩니다. 만약 오류가 미리 정해놓은 기준치(threshold)보다 크다면, 우리는 그것을 이상거래로 판단합니다.(정상 거래 데이터에서는 우리의 모델이 작은 오류를 가질 것이기 때문에) 그럼 한 번 기준치를 정해볼까요:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYRJR7B0jwbv"},"outputs":[],"source":["threshold = 2.9"]},{"cell_type":"markdown","metadata":{"id":"NB3dACiwjwbv"},"source":["두 가지 유형의 거래를 얼마나 잘 나누는지 확인해봅시다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AK-Z6hWMjwbv"},"outputs":[],"source":["groups = error_df.groupby('true_class')\n","fig, ax = plt.subplots()\n","\n","for name, group in groups:\n","    ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n","            label= \"Fraud\" if name == 1 else \"Normal\")\n","ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n","ax.legend()\n","plt.title(\"Reconstruction error for different classes\")\n","plt.ylabel(\"Reconstruction error\")\n","plt.xlabel(\"Data point index\")\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"y4fk70cQjwbv"},"source":["차트만으로는 이해하기 힘들 것 같네요. Confusion matrix(분류한 결과를 보여주는 행렬)를 확인해봅시다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVP_hy8Jjwbv"},"outputs":[],"source":["y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n","conf_matrix = confusion_matrix(error_df.true_class, y_pred)\n","plt.figure(figsize=(12, 12))\n","sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n","plt.title(\"Confusion matrix\")\n","plt.ylabel('True class')\n","plt.xlabel('Predicted class')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"nUs2aMnZjwbw"},"source":["우리의 모델이 많은 이상 거래 데이터를 잡아낸 것으로 보입니다. 물론, 문제점도 있긴 합니다. 정상 거래 데이터 중 이상 거래 데이터로 분류된 것들이 꽤 많습니다. 이게 진짜 문제일까요? 그렇겠죠. 아마 독자님의 문제에 따라 기준치를 높이거나 낮추고 싶을 겁니다. 그리고 그렇게 하는 건 독자님의 판단에 달렸습니다."]},{"cell_type":"markdown","metadata":{"id":"l3ZqjdMmjwbw"},"source":["## 결론\n","\n","우리는 지금까지 Keras로 정상 거래 기록을 재구성하는 매우 간단한 Deep Autoencoder를 만들어보았습니다.  우리는 수많은 단일 클래스 예시(정상 거래 기록)들만 주었을 뿐인데 모델이 어떻게든 학습해서 새로운 것이 같은 클래스인지 아닌지를 구별할 수 있게 되었습니다. 멋지지 않은가요? 우리의 데이터셋이 마치 마법 같았죠. 우리는 심지어 원래 데이터가 어떻게 생겼는지도 모릅니다.(이미 한 차례 PCA 변환을 거쳤기 때문에)\n","\n","Keras는 우리에게 자칫 어려울 수도 있는 Deep Autoencoder를 매우 깔끔하고 쉽게 만들 수 있는 API를 제공합니다. Tensorflow 구현체를 찾고 하나를 학습시키기 위해 얼마나 많은 상용코드(boilerplate code, 최소한의 수정만을 거쳐 여러 곳에 필수적으로 사용되는 코드)가 필요한지 직접 확인하는 방법도 있긴 합니다. 자, 그럼 이제 비슷한 모델을 다른 문제에 적용해 볼까요?\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fzvqMNhljwbw"},"source":["## 참조\n","\n"," - https://blog.keras.io/building-autoencoders-in-keras.html\n"," - http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/\n"," - http://cmgreen.io/2016/01/04/tensorflow_deep_autoencoder.html"]},{"cell_type":"markdown","metadata":{"id":"s3iWlxasjwbw"},"source":["### 저장된 모델 사용하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GLTvV7H3jwbw"},"outputs":[],"source":["autoencoder = load_model('model.h5')"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.3"}},"nbformat":4,"nbformat_minor":0}